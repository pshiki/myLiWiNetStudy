

2.1.2 Запускаем Pod
Создать pod
kubectl create -f pod.yaml
Посмотреть поды
kubectl get pods
Удалить pods 
kubectl delete pod --all

2.1.3 Практика
Запустите в кластере pod из образа busybox:latest c командой sh -c 'while true; do echo New random number is $(( ( RANDOM % 100 )  + 1 )); sleep 2; done'  и именем hello

cd  ~/slurm/practice/2.application-abstractions/1.pod
kubectl create -f pod.yaml
vim pod.yaml
kubectl create -f pod.yaml
kubectl get pod
kubectl delete pod --all

2.1.4 Практика
  846  cp pod.yaml pod-sam.yaml
  850  kubectl create -f pod-sam.yaml
  852  kubectl logs hello-pod
  853  vim pod-sam.yaml
  856  kubectl delete pod hello-pod

---
# file: practice/2.application-abstractions/1.pod/pod-sam.yaml
apiVersion: v1
kind: Pod
metadata:
  name: hello
spec:
  containers:
  - image: busybox:latest
    name: hello
    command: ["/bin/sh"] 
    args: ["-c", "while true; do echo New random number is $(( ( RANDOM % 100 )  + 1 )); sleep 2; done"]
...



Посмотреть логи pod'а:
kubectl logs hello

Удалить конкретный pod:
kubectl delete pod hello







2.2.1 Replica set 

Replica set — это важная абстракция в Kubernetes, которая позволяет масштабировать поды внутри кластера.
Replica set фактически является шаблоном, который описывает темплейт для создания подов. В этом шаблоне можно указать количество подов, которые нужно создать.

В спецификации (spec) replica set содержится несколько важных полей:
replicas — указывает, сколько реплик (подов) нужно создать из данного шаблона. Например, если в поле указано значение 2, это означает, что необходимо запустить две реплики.
selector — включает поле matchLabels, где можно определить метки (лейблы), которые будут использоваться для выбора подов. Например, лейбл app-myapp.
Селектор matchLabels в replica set определяет, что replica set будет управлять всеми подами, у которых установлен лейбл app: my-app

Лейблы используются для структурирования и логической группировки объектов. Так, лейблы они могут применяться для обозначения уровней (tiers) таких как frontend, backend, database, или по именам приложений.

replica set следит за всеми подами, у которых лейбл app: my-app если в селекторе находится этот match label

показать все поды, у которых лейбл app равен my-app:
kubectl get pod -l app: my-app     // Это позволяет эффективно управлять объектами и выполнять выборку на основе меток.

Перед применением нового replica set рекомендуется удалить старые поды.

Применить(создать, запустить) replica set:
kubectl create -f replicaset.yml

сокращение:
replica set - rs

kubectl get rs будет работать так же, как и kubectl get replica set


Масштабирование replica set
Первый способ:
1) Обновить количество реплик в конфигурационном файле replica-set.yaml
2) kubectl apply -f replicaset.yaml
apply != create

Второй способ:
kubectl scale replicaset my-replicaset --replicas 3


Обновление версий приложений в Kubernetes
Первый способ:
1) внести изменеия в в файл replicaset.yaml
2) kubectl apply -f replicaset.yaml

Второй способ:
kubectl edit replicaset my-replicaset
Таким образом, изменения вносятся непосредственно в кластер в реальном времени, но применены они не будут, для применения нужно что бы под пересоздался.
При использовании команды edit изменения вносятся в удаленный объект в кластере и не сохраняются в локальном файле конфигурации, таком как replicaset.yaml

Третий способ:
kubectl set image replicaset my-replicaset nginx=nginx:1.21

поды не были обновлены автоматически, поскольку replica set предназначен для поддержания заданного количества реплик, но не для обновления уже существующих подов после изменения темплейта

Получение подробной информации о поде:
kubectl describe pod <podname>
Получение подробной информации о replicaset:
kubectl describe replicaset <replicasetName>

Можно сделать describe любого объекта в кластере Kubernetes. Эта команда предоставляет базовое описание объекта, включая информацию о полях и событиях (events), произошедших с объектом в кластере Kubernetes.

kubectl get pod <podname> -o=jsonpath='{.spec.containers[*].image}{"\n"}'
Ключ -o jsonpath позволяет получить не весь объект целиком, а только содержимое конкретных полей.
Он крайне полезен при написании скриптов для автоматизации задач в Kubernetes.


2.2.3 Практика
  860  cd k8s_slurm/slurm_devk8s/practice/2.application-abstractions/2.replicaset
  863  kubectl apply -f replicaset.yaml
  865  kubectl scale rs my-replicaset --replicas 3
  867  kubectl delete pod my-replicaset-rvxpn
  869  kubectl set image rs my-replicaset nginx=nginx:1.21
  870  kubectl describe rs my-replicaset
  872  kubectl describe pod my-replicaset-bj8tl
  873  kubectl delete pod my-replicaset-bj8tl
  874  kubectl get pod
  875  kubectl describe pod my-replicaset-bwsm2
  876  kubectl get pod my-replicaset-bwsm2 -o=jsonpath='{.spec.containers[*].image}{"\n"}'
  877  kubectl delete rs --all
  878  kubectl get rs
  879  kubectl get pods







2.3.1 Deployment 

Deployment представляет собой уровень абстракции над ReplicaSet и используется для управления обновлениями версий приложений. Основная задача Deployment — создание новых ReplicaSet с обновленными версиями приложения, которые, в свою очередь, создают необходимые поды.


Проверим создание Deployment, применив команду:
kubectl get deployment

Чтобы изменить число реплик, можно отредактировать соответствующий файл Deployment или применить команду:
kubectl scale


Теперь попробуем обновить наш Deployment. Для этого существует несколько способов:
1 Изменение версии образа в файле с описанием Deployment и его применение через команду kubectl apply
2 Использование команды kubectl edit deployment my-deployment для редактирования ресурса непосредственно в кластере:
3 Использование команды kubectl set image deployment my-deployment 'nginx=nginx:1.13'

Стратегия Rolling Update
Rolling Update — стратегия обновления, поддерживаемая Kubernetes по умолчанию. Она обеспечивает постепенное обновление подов, гарантируя плавный переход между версиями приложения. Суть Rolling Update заключается в том, что старые реплики заменяются новыми поэтапно: сначала часть старых реплик отключается, затем на их месте запускаются новые. Такой процесс не прерывает работу приложения и остается незаметным для пользователей, поскольку всегда есть активные поды, обрабатывающие запросы.
Процесс управления Rolling Update в Deployment позволяет контролировать, сколько старых реплик удаляется за раз и сколько новых запускается одновременно. Эти параметры могут быть настроены в конфигурации Deployment.
В описании конфигурации Deployment: (spec) можно задать дополнительные параметры, такие как стратегия обновления (strategy) это позволяет управлять процессом обновления приложения, задавая количество одновременно обновляемых и сохраняемых реплик во время update. В поле strategy у Deployment можно задать два типа стратегий обновления: RollingUpdate и Recreate

Тип RollingUpdate, как уже было рассмотрено ранее, используется по умолчанию и подразумевает постепенное обновление подов: сначала создаются новые поды, а затем старые поды удаляются. Этот метод обновления обеспечивает непрерывность работы приложения, хотя требует дополнительных ресурсов, так как некоторое время новые и старые поды будут работать одновременно.

Внутри стратегии RollingUpdate предусмотрены два важных параметра: maxSurge и MaxUnavailable

MaxSurge определяет, на сколько новых реплик можно увеличить текущее количество подов относительно заданного значения replicas во время обновления. Например, если у нас replicas равно 2, то при MaxSurge: 1 Kubernetes может поднять одну дополнительную реплику во время обновления. В итоге, на время обновления у нас будет три работающих пода — это текущие 2 пода плюс одна дополнительная реплика.
MaxUnavailable указывает, сколько подов может быть недоступно во время обновления. В нашем случае, если MaxUnavailable: 1, это означает, что как только процесс обновления запустится, Kubernetes может сразу же удалить одну старую реплику, оставив временно только одну рабочую.

Вместо числовых значений, можно указать проценты. Например, можно настроить MaxSurge и MaxUnavailable как 10%

Стратегия Recreate, в свою очередь, предполагает полное удаление всех старых подов перед созданием новых. Это подход, при котором приложение будет временно недоступно (downtime), что делает его менее подходящим для production-сред. Однако такой способ может оказаться полезным в ситуациях, когда недопустима одновременная работа старой и новой версий приложения, например, из-за различий в обработке данных. Он также может быть полезен в dev-средах, где важна экономия ресурсов, поскольку в процессе обновления не требуется дополнительного места для одновременной работы двух наборов подов.

Если необходимо использовать стратегию обновления, но формат заполнения поля strategy неизвестен, Kubernetes предоставляет удобный инструмент для получения информации — команду kubectl explain. Она позволяет выводить описание полей любых объектов в кластере.
kubectl explain deployment.spec.strategy....

kubectl explain - это man по k8s

INFO
PODs и ReplicaSet не используются напрямую, они являются больше служебными абстракциями
Конекретно используется: Deployment, которы включается в себя ReplicaSet, в свою очередь, который включает в себя PODs

kubectl apply лучше использовать всегда (даже вместо kubectl create)


2.3.3 Практика

#  887  minikube start --driver=vmware
  888  history 1 | tail -n 100
  889  history | tail -n 100
  890  vim deployment.yaml
  891  kubectl apply -f deployment.yaml
  892  kubectl get pods
  894  kubectl set image deployment my-deployment nginx=nginx:1.21
  895  kubectl get pod
  896  kubectl get rs
  897  kubectl get deployments
  898  kubectl describe pod my-deployment-59659b5568-55czr
  899  kubectl get deployment my-deployment -o=jsonpath='{.status.conditions[1].message}{"\n"}'
  902  kubectl delete deployments.apps my-deployment


2.3.4 Практика

cp deployment.yaml deployment-update.yaml
vim deployment-update.yaml

---
# file: practice/2.application-abstractions/3.deployment/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
...

kubectl apply -f deployment-update.yaml
kubectl get pod
vim deployment-update.yaml
kubectl apply -f deployment-update.yaml
kubectl get pod
kubectl get deployment my-deployment -o custom-columns='NAME:.metadata.name,MAXSURGE:.spec.strategy.rollingUpdate.maxSurge,MAXUNAVAILABLE:.spec.strategy.rollingUpdate.maxUnavailable'
kubectl delete deployments.apps my-deployment

Это еще одна возможность ключа -o. Она позволяет вывести описание объекта с пользовательским набором полей.





2.4.1 Namespaces

Namespace — это базовая абстракция кластера Kubernetes, представляющая собой пространство имен. Основная функция namespace заключается в разделении имен объектов внутри кластера. В пределах одного namespace невозможно создать два объекта одного типа с одинаковым именем. 


По умолчанию работа в кластере ведется в namespace Default. 

для указания другого namespace, можно использовать команду: 
kubectl -n
при выполнении команд get, create, apply, delete и других и работать с объектами в другом namespace. Таким образом, namespace обеспечивает разделение пространства имен.


2.4.3 Практика

kubectl -n kube-system get pod
kubectl create ns student
kubectl -n student apply -f deployment.yaml
kubectl -n student get pod

kubectl delete deployments.apps -n student --all

kubectl delete deployment --all






2.5.1 Resources

В Kubernetes ресурсы делятся на два типа:

    Реквесты (Requests) — это минимальное количество ресурсов, которое будет зарезервировано под приложение в кластере. Эти ресурсы гарантированно будут доступны приложению.
    Лимиты (Limits) — это максимальное количество ресурсов, которое под может использовать. Лимиты устанавливают верхнюю границу, выше которой ресурсы не будут выделяться.

При превышении лимитов, например, при попытке приложения использовать больше оперативной памяти, включается механизм Out of Memory Killer (OOM Killer) для контейнера

В соотвествии с указанными Requests кубер раскидывает pods по workers кластера

Проц: 1 ядро = 1000m (мили цпу)
cpu: 100m это 1\10 одного ядра (процессорного времени)

Память: 1Gi = 1024 мегабайт
        1GB = 1000 мегабайт

По соотношению request и limmit кубер пристваивает QoS Class, который можно увидеть в describe

QoS Class:
Garanteed - request = limmit (самый приоритетный)
Этот класс применяется, когда реквесты и лимиты равны между собой для CPU и памяти. В таком случае поду гарантируется выделение всех запрошенных ресурсов. Поды с классом Guaranteed имеют наивысший приоритет и сохраняются на ноде в первую очередь при возникновении дефицита ресурсов.

Burstable - request < limmit
Этот класс применяется, когда реквесты меньше, чем лимиты. В некоторых случаях под может использовать и больше ресурсов, если они доступны. Например, если приложение зарезервировало себе 100 mCPU, но при наличии свободных ресурсов оно может использовать до 200 mCPU, если в лимите указано это значение. Однако такие поды имеют более низкий приоритет, чем Guaranteed, то есть при нехватке ресурсов на ноде Kubernetes сначала начнет переселять поды класса Burstable на другие ноды, в то время как поды класса Guaranteed будут сохраняться дольше всего.

Bestapport - no limmit
Этот класс назначается, когда лимиты не заданы. Приложение может использовать любые доступные ресурсы, но при этом имеет наименьший приоритет. В случае дефицита ресурсов на ноде, поды класса Best Effort будут переселяться в первую очередь.


info:
kubectl delete all --all-namespaces --all   =   kubectl delete all -A --all   =   rm -rf /



2.5.3 Практика

cd ~/slurm/practice/2.application-abstractions/4.resources/
kubectl apply -f deployment-with-resources.yaml
kubectl get pod
kubectl patch deployment my-deployment --patch '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","resources":{"requests":{"cpu":"10"},"limits":{"cpu":"10"}}}]}}}}'
kubectl get pod
kubectl describe pod my-deployment-5b98d9cdff-jnwqv

kubectl delete deployment --all





3.1.1 Переменные окружения

Это первый способ конфигурирования приложения

В рамках деплоймента можно передавать настройки приложению, а именно конфигурировать переменные окружения. Для этого у деплоймента есть конкретная секция, называемая env. В этой секции можно перечислить любое количество переменных окружения и задать им значения, например, DB_HOST, DB_PORT, DB_USER, DB_PASSWORD и так далее.
Конфигурирование происходит очень просто и понятно. В секции env обязательно должны быть две строки: name и value. В name указывается название переменной окружения, например, DB_HOST. В нашем случае название переменной ‘TEST’.  В value — значение этой переменной, например, IP-адрес или доменное имя. Если имя переменной окружения DB_USER, то в value пишется имя этого пользователя и т.д.

env:
- name: foo
  value: bar


Практика 3.1.3
kubectl apply -f deployment-with-env.yaml
kubectl get pods
kubectl describe pod my-deployment-77f7c7bcf5-vblld
kubectl exec -it my-deployment-77f7c7bcf5-vblld -- env
kubectl delete deployments.apps --all







3.2.1 ConfigMAp

Это второй способ конфигурирования приложения

Один configMap можно подключать к различным deployments (например: распространить настройки на другие приложения)

configmap записываются в отдельный *.yaml файл и подключаются в нужный файл приложения (deployment.yaml). Для подключения ConfigMap к приложению используется поле EnvFrom.

Env и ConfigMap можно использовать совместно


cat configmap.yaml                      ✔ 
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap-env
data:
  dbhost: postgresql
  DEBUG: "false"
...

cat deployment-with-env-cm.yaml         ✔ 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        env:
        - name: TEST
          value: foo
        envFrom:                           #подключаем configMap файл
        - configMapRef:
            name: my-configmap-env
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
...


kubectl create -f configmap.yaml
kubectl get cm                 #cm - configmap - onfigmaps
kubectl describe cm my-configmap-env

kubectl get cm my-configmap-env -o yaml

kubectl get deployments.apps

Если внести изменения в configMap и даже применить их, они в уже работающих deployments не отразятся без рестарта


Практика 3.2.3

kubectl create -f configmap.yaml
kubectl get configmaps
kubectl describe cm my-configmap-env
kubectl get cm my-configmap-env -o yaml
kubectl apply -f deployment-with-env-cm.yaml
kubectl exec -it my-deployment-7b5b4c5c99-tq97j -- env
kubectl delete deployments.apps --all






3.3.1 Secrets


Secret поддерживает несколько типов для разных задач:
Generic Secret — используется для хранения токенов, паролей и другой конфиденциальной информации.
Docker-registry Secret — специальный тип для передачи данных авторизации в Docker Registry
TLS Secret — используется для хранения сертификатов для подключения TLS, например, для установления HTTPS-соединений.

Создание секрета:

1 вариант
kubectl create secret generic mysecrets --from-literal='foo=bar' --from-literal='dbpasswd=1q2w3e'

Смотрим какие есть секреты:
kubectl get secrets
kubectl describe secrets  mySecrets
kubectl get secrets mysecrets -o yaml

Секреты выводятся закодированными в Base64


Для надёжного хранения секретов надо использовать vault


Подключение секретов:
Secret можно подключить двумя способами:

1 Целиком — с использованием секции envFrom, что позволит подключить весь Secret целиком ко всем переменным окружения.
2 Избирательно — с использованием секции valueFrom, что позволяет передать значение из Secret только для конкретной переменной.

В манифесте 2:
env:
- name: myVar
  valueFrom:
    secretKeyRef:
      name: mySecrets
      key: foo


В манифесте 1:
envFrom:
  - secretRef:
      name: intercom-secret
  - secretRef:
      name: paypal-secret
  - secretRef:
      name: postgres-secret
  - secretRef:
      name: redis-secret




2 вариант

secret-base64.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: test-secret
data:
  username: bXktYXBw
  password: Mzk1MjgkdmRnN0pi



secret-plaintext.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: test-secret
stringData:
  username: paul
  password: mYSuPeRPass123





stringData - plain text format
data - base64 format



kubectl apply -f secret.yaml




3.3.1 Практика

kubectl create secret generic mySecrets --from-literal='foo=bar' --from-literal='dbpasswd=1q2w3e'
kubectl get secrets
kubectl get secrets -o yaml
kubectl get secret mysecrets -o yaml
kubectl apply -f deployment-with-secret.yaml
kubectl get pod
kubectl describe pod my-deployment-5bf6449f4-jhws8
kubectl apply -f secret.yaml
kubectl get secrets -o yaml






3.4.1 configMap. Часть 2

Актуально для Legacy приложений, которые не воспринимают переменными, например: конфигурация требуется отдельным форматом, отличном от YAML, JSON или ключ-значение

Пример такого configMap:
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  default.conf: |
    server {
        listen       80 default_server;
        server_name  _;

        default_type text/plain;

        location / {
            return 200 '$hostname\n';
        }
    }
...


Пример подключения такого configMap

cat deployment-with-configmap.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/conf.d/
      volumes:
      - name: config
        configMap:
          name: my-configmap
...


Секция Volumes предназначена для подключения различных томов (volumes) к приложению (deployment)
Секция Volumes находится на уровне spec (спецификации) всего деплоймента, а не внутри контейнеров. Это связано с тем, что один и тот же деплоймент может использовать несколько томов и контейнеров одновременно. В итоге, можно подключать разные типы томов к различным контейнерам, а также использовать один и тот же том в нескольких контейнерах. Поэтому тома конфигурируются отдельно в секции Volumes, где описываются все используемые тома, их типы и названия.

VolumeMounts — это секция, которая непосредственно связывает тома из секции Volumes с конкретными контейнерами. В этой секции указывается, какой именно том используется и куда он будет монтирован внутри контейнера.

Volumes - что монтируем
VolumeMounts - кому и куда монтируем

может быть использован не только ConfigMap, но и другие типы томов, включая Secrets

файл default.conf — это символическая ссылка, которая указывает на другой файл, data-default.conf. При этом ..data — это символическая ссылка, ведущая к директории с таймстемпом, поэтому подключенные данные через Volume, можно изменять на лету (без перезагрузки POD), однако некоторые приложения требую рестарт, что бы применить изменённые напирмер конфиги. Изменения происходят за счет симлинков

Проброс порта
kubectl port-forward my-deployment-543fgfdgdf-fsd43d 8080:80 &
curl localhost:8080
Это нужно если необходимо получить данные из контейнера (curl), который находится не в одной сети с кластером
Отключить: fg bg или kill -9 $(ps -ef | grep 'port-forward' | awk '{print $2}')



3.4.3 Практика

kubectl apply -f configmap.yaml
kubectl apply -f deployment-with-configmap.yaml
kubectl get pods  -o wide
curl 10.224.0.34
kubectl describe cm my-configmap
kubectl describe pod my-deployment-678fc95fc8-hfdd5
kubectl exec -it my-deployment-678fc95fc8-hfdd5 -- bash
kubectl port-forward my-deployment-678fc95fc8-hfdd5 8080:80 &
curl localhost:8080


3.4.6 Практика

vim configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  default.conf: |
    server {
        listen       80 default_server;
        server_name  _;

        default_type text/plain;

        location / {
            return 200 '$hostname\nOK\n';
        }
    }
...

curl localhost:8080
kubectl exec -it my-deployment-678fc95fc8-hfdd5 -- bash
cat /etc/nginx/conf.d/default.conf



3.4.7 Практика (херня полная, вообще непонятно)

kubectl apply -f php-config.yaml
kubectl apply -f deployment-with-configmap-php.yaml
kubectl get pods
cat php-config.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-php-conf
data:
  php.ini: |
    [php]
    register_globals = off
    track_errors = yes
...

cat deployment-with-configmap-php.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: quay.io/testing-farm/nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/conf.d/
        - name: php
          mountPath: /etc/php.ini
          subPath: php.ini
      volumes:
      - name: config
        configMap:
          name: my-configmap
      - name: php
        configMap:
          name: my-php-conf
...
kubectl exec -it my-deployment-798c5bd979-ks68h -- bash
ls -lsa /etc          # тут типо что-то не так
kubectl edit deployments my-deployment   # типо чиним Убираем subPath, меняем точку монтирования.   
  - mountPath: /etc
    name: php

kubectl get pod   #под не поднимается 
kubectl describe pod my-deployment-6f74684dd-wd46w
kubectl delete deployments my-deployment






3.5.1 Downward API

Этот метод не столько предназначен для внесения пользовательских конфигураций, сколько для передачи параметров окружения, в котором работает приложение, включая параметры, касающиеся самого Kubernetes. С помощью Downward API можно передать внутрь контейнера определённые параметры манифеста Kubernetes в виде переменных окружения или файлов.

Для получения какого-либо значения из манифеста, например, имя ноды или другого параметра из Kubernetes, используем следующую конструкцию:

- name: __NODE_NAME

valueFrom: показывает, что значение для переменной мы не задаём вручную, а берём из другого источника.
fieldRef: позволяет сослаться на конкретное поле манифеста.
fieldPath: указывает конкретное место в манифесте в формате YAML, откуда будет взято значение. Это значение берется непосредственно из манифеста.


Если требуется получить данные из более глубоких уровней манифеста, можно использовать точечную нотацию YAML, например, spec.resources.limits, чтобы получить лимиты ресурсов.

Также можно передать в контейнер информацию о сервис-аккаунте, под которым работает под. 
Данный Volume имеет тип downwardAPI
В секции items указываем, какие именно данные будут переданы через этот Volume - 'labels’, содержащий все лейблы манифеста, и 'annotations', в который записываются все аннотации манифеста.
В файле labels все метки, ассоциированные с данным манифестом.
В файле annotations записаны все аннотации, относящиеся к этому поду
Аннотации представляют собой дополнительные метки, используемые для реализации определённого функционала в Kubernetes, используются в Ingress

cat deployment-with-downward-api.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        env:
        - name: TEST
          value: foo
        - name: __NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: __POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: __POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: __POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: __NODE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        - name: __POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/conf.d/
        - name: podinfo
          mountPath: /etc/podinfo
      volumes:
      - name: config
        configMap:
          name: my-configmap
      - name: podinfo
        downwardAPI:
          items:
            - path: "labels"
              fieldRef:
                fieldPath: metadata.labels
            - path: "annotations"
              fieldRef:
                fieldPath: metadata.annotations
...

https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/
apiVersion: v1
kind: Pod
metadata:
  name: kubernetes-downwardapi-volume-example
  labels:
    zone: us-est-coast
    cluster: test-cluster1
    rack: rack-22
  annotations:
    build: two
    builder: john-doe
spec:
  containers:
    - name: client-container
      image: registry.k8s.io/busybox
      command: ["sh", "-c"]
      args:
      - while true; do
          if [[ -e /etc/podinfo/labels ]]; then
            echo -en '\n\n'; cat /etc/podinfo/labels; fi;
          if [[ -e /etc/podinfo/annotations ]]; then
            echo -en '\n\n'; cat /etc/podinfo/annotations; fi;
          sleep 5;
        done;
      volumeMounts:
        - name: podinfo
          mountPath: /etc/podinfo
  volumes:
    - name: podinfo
      downwardAPI:
        items:
          - path: "labels"
            fieldRef:
              fieldPath: metadata.labels
          - path: "annotations"
            fieldRef:
              fieldPath: metadata.annotations


https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/
apiVersion: v1
kind: Pod
metadata:
  name: dapi-envars-fieldref
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "sh", "-c"]
      args:
      - while true; do
          echo -en '\n';
          printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE;
          printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;
          sleep 10;
        done;
      env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: MY_POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
  restartPolicy: Never


3.5.3 Практика

kubectl apply -f deployment-with-downward-api.yaml
kubectl exec -it my-deployment-8f7dd6848-kktb6 -- env

kubectl exec -it my-deployment-8f7dd6848-kktb6 -- cat /etc/podinfo/labels
kubectl exec -it my-deployment-8f7dd6848-kktb6 -- cat /etc/podinfo/annotations

kubectl delete deployment my-deployment
kubectl delete configmap my-configmap-env
kubectl delete configmap my-configmap
kubectl delete secret mysecrets








4.1.1 Хранение данные

k8s не для хранения данных. Данные надо хранить в базе данных (отдельные машины). Файлы надо хранить в S3.

пока мы знаем только два типа volumes: confgMap и secret



4.2.1 HotsPath

Это обычное монтирование директории или диска на сервере в контейнер
НО! это опасно!!! При использовании hotsPath пользователь может замонтировать какую угодно директории, например /etc/kubernetes
в котором находятся сертификаты и админские токены...
ДАнный тип нужен для корректного функционирования самого k8s кластера

На PROD это использовать нельзя!

ограничения возможны через Pod Security Admission Controller или Open Policy Agent Gatekeeper, которые позволяют контролировать и ограничивать применение HostPath, оставляя его только для необходимых системных компонентов.

Пример hotsPath


        volumeMounts:
        - name: data
          mountPath: /files
      volumes:
      - name: data
        hostPath:
          path: /data_pod

Каталог с хостовой машины (node (worker) на котором будет запущен Pod) /data_pod будет примонтирован внутри контейнера.
Точка монтирования указана в разделе VolumeMount. Здесь мы указываем, что Volume с именем data следует монтировать в каталог /files внутри контейнера


4.2.3 Практика

cat deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: data
          mountPath: /files
      volumes:
      - name: data
        hostPath:
          path: /data_pod
...



cd ~/slurm/practice/4.saving-data/1.hostpath/
kubectl apply -f deployment.yaml

kubectl get pod
kubectl get all
kubectl describe rs my-deployment-6468989d7d


kubectl delete deployment my-deployment








4.3.1 EmptyDir

В отличии от HostPath, данный тип Volume является более безопасным и чаще применяется в различных сценариях, не вызывая серьезных вопросов, связанных с безопасностью. Volume EmptyDir со всеми данными существует только в течение жизненного цикла пода. Данный тип volume отлично подходит для тестов

Пример:

        volumeMounts:
        - name: data
          mountPath: /files
      volumes:
      - name: data
        emptyDir: {}



4.3.3 Практика

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: data
          mountPath: /files
      volumes:
      - name: data
        emptyDir: {}
...


kubectl apply -f deployment.yaml
kubectl exec -it my-deployment-7db8d46cf-bd75c -- sh -c 'echo "Some data" > /files/data.txt'
kubectl exec -it my-deployment-7db8d46cf-bd75c -- cat /files/data.txt
kubectl delete pod my-deployment-7db8d46cf-bd75c
kubectl exec -it my-deployment-7db8d46cf-bd75c -- sh -c 'cat /files/data.txt'
kubectl delete deployments.apps my-deployment










4.4.1 PV\PVC

PersistentVolumeClaim (PVC) - В рамках Persistent Volume Claim мы описываем требования и пожелания к тому, какой диск или хранилище, волюм нужно подключить к приложению. Например, здесь можно указать:
    Размер: Желаемый объем того Volume, который должен быть доступен в контейнере.
    Тип хранилища: Если в кластере имеется несколько хранилищ, можно выбрать конкретное (Ceph или NFS и т.д.).
    Метод доступа: Способ взаимодействия с подключаемым диском или Volume.
Persistent Volume Claim — это объект, в котором определяются параметры и пожелания к подключаемому диску или Volume
создается отдельно или заранее.


Persistent Volume (PV) - это описание параметров и статуса тома, используемого в кластере

Storage Class - хранит настройки подключения к хранилищу
предоставляет информацию о том, где находится NFS-сервер, какие логины и пароли использовать, как подключаться, и так далее
Это уже уровень ПРОДа


Можно вручную создавать PVs - Pool of PVs, или использовать PV Provisioners - это автоматическое получение томов в k8s ровно необходимого места с СХД
Provisioner следит за запросами PVC. Когда возникает новый запрос, Provisioner автоматически обращается к СХД, создает необходимые тома и подключает их к k8s, формируя PV. Кроме того, Provisioner создаёт тома в точном соответствии с размером, указанным в PVC


Пример создания Storage Class и Volume без использования автоматизации через Provisioner
initContainers — контейнер, который запускается до основных контейнеров и выполняет предварительную настройку

PVC info
Существует два основных типа доступа к Volume:
1. ReadWriteMany — позволяет множественное чтение и запись
2. ReadWriteOnce — ограничивает доступ только для одного источника (поды одной ноды)


SС info
в качестве Provisioner указан NoProvisioner - означаент встроенный, ручное создание PV
Storage Class требует ручного вмешательства, так как он действует по принципу volumeBlindingMode: waitForFirstConsumer


PV info
Volume Mode — File System
Access Mode:
 - ReadWriteOnce

Опция Local указавает, что этот Volume будет использоваться как локальная папка
путь к этой папке через localPath, через объект nodeAffinity и nodeSelector задаем конкретную ноду, на которой будет создан данный Volume

***
Спецификация Storage Class - тут остановился
***

В разделе Data Source укажем StorageClassName (для точной привязки PVC к конкретному Storage Class и, соответственно, к определенному SHD в PVC рекомендуется явно указывать спецификацию Storage Class)

В отличие от hostPath каталоги автоматически не создаются на хостовой ноде, поэтому их необходимо создавать вручную


PV статус 'Bound' означает успешное монтирование



Автоматизация создания PV через NFS
Помимо ручной конфигурации в StorageClass с параметром NoProvisioner, как мы уже рассмотрели, можно настроить автоматическое создание PV через подключение к сетевому хранилищу, например, NFS. В этом случае, когда создается PVC, Provisioner автоматически запрашивает необходимое пространство в хранилище, создаёт PV и монтирует его в кластер. Таким образом, ручные действия не требуются





Пример ручной нарезки дисков

cat pvc.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: fileshare
spec:
  storageClassName: local-storage
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


cat deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fileshare
spec:
  replicas: 2
  selector:
    matchLabels:
      app: fileshare
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: fileshare
    spec:
      initContainers:
      - image: busybox
        name: mount-permissions-fix
        command: ["sh", "-c", "chmod 777 /data"]
        volumeMounts:
        - name: data
          mountPath: /data
      containers:
      - image: centosadmin/reloadable-nginx:1.12
        name: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/conf.d
        - name: data
          mountPath: /data
      volumes:
      - name: config
        configMap:
          name: fileshare
      - name: data
        persistentVolumeClaim:
          claimName: fileshare


cat configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fileshare
data:
  default.conf: |
    server {
      listen       80 default_server;
      server_name  _;

      default_type text/plain;

      location / {
        return 200 '$hostname\n';
      }

      location /files {
        alias /data;
        autoindex on;
        client_body_temp_path /tmp;
        dav_methods PUT DELETE MKCOL COPY MOVE;
        create_full_put_path on;
        dav_access user:rw group:rw all:r;
      }
    }

cat sc.yaml
---
kind: StorageClass  #ручное создание
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

cat pv.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: node1-pv1
spec:
  capacity:
    storeage: 5Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /local/pv1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1.s0001.slurm.io



kubectl get pvc
kubectl get pv
kubectl get sc

kubectl create -f sc.yaml
kubectl create -f pv.yaml

ssh root@node1.s0001.slurm.io
mkdir -p /local/pv1




4.4.3 Практика

kubectl apply -f pvc.yaml
kubectl get pvc
kubectl get pv
kubectl apply -f .
kubectl describe pod fileshare-9989bfb55-f6q6v
kubectl exec -it fileshare-9989bfb55-f6q6v -- df -h
kubectl delete -f .






4.5.1 Init Container
В секции initContainers можно задать контейнеры, которые будут выполняться перед основными контейнерами, описанными в секции containers. Например, в containers может запускаться наш Nginx, а перед этим, в init-контейнерах, могут выполняться другие задачи. Например, они могут установить необходимые права на каталог, если приложение должно работать от непривилегированного пользователя, или как пример копирование конфигурационных файлов, установка прав или создание каталогов.
Init-контейнеры выполняются последовательно в том порядке, в котором они описаны в манифесте.
init-контейнеры завершают свою работу перед тем, как будут запущены основные контейнеры.























































