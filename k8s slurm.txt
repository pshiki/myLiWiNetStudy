

2.1.2 Запускаем Pod
Создать pod
kubectl create -f pod.yaml
Посмотреть поды
kubectl get pods
Удалить pods 
kubectl delete pod --all

2.1.3 Практика
Запустите в кластере pod из образа busybox:latest c командой sh -c 'while true; do echo New random number is $(( ( RANDOM % 100 )  + 1 )); sleep 2; done'  и именем hello

cd  ~/slurm/practice/2.application-abstractions/1.pod
kubectl create -f pod.yaml
vim pod.yaml
kubectl create -f pod.yaml
kubectl get pod
kubectl delete pod --all

2.1.4 Практика
  846  cp pod.yaml pod-sam.yaml
  850  kubectl create -f pod-sam.yaml
  852  kubectl logs hello-pod
  853  vim pod-sam.yaml
  856  kubectl delete pod hello-pod

---
# file: practice/2.application-abstractions/1.pod/pod-sam.yaml
apiVersion: v1
kind: Pod
metadata:
  name: hello
spec:
  containers:
  - image: busybox:latest
    name: hello
    command: ["/bin/sh"] 
    args: ["-c", "while true; do echo New random number is $(( ( RANDOM % 100 )  + 1 )); sleep 2; done"]
...



Посмотреть логи pod'а:
kubectl logs hello

Удалить конкретный pod:
kubectl delete pod hello







2.2.1 Replica set 

Replica set — это важная абстракция в Kubernetes, которая позволяет масштабировать поды внутри кластера.
Replica set фактически является шаблоном, который описывает темплейт для создания подов. В этом шаблоне можно указать количество подов, которые нужно создать.

В спецификации (spec) replica set содержится несколько важных полей:
replicas — указывает, сколько реплик (подов) нужно создать из данного шаблона. Например, если в поле указано значение 2, это означает, что необходимо запустить две реплики.
selector — включает поле matchLabels, где можно определить метки (лейблы), которые будут использоваться для выбора подов. Например, лейбл app-myapp.
Селектор matchLabels в replica set определяет, что replica set будет управлять всеми подами, у которых установлен лейбл app: my-app

Лейблы используются для структурирования и логической группировки объектов. Так, лейблы они могут применяться для обозначения уровней (tiers) таких как frontend, backend, database, или по именам приложений.

replica set следит за всеми подами, у которых лейбл app: my-app если в селекторе находится этот match label

показать все поды, у которых лейбл app равен my-app:
kubectl get pod -l app: my-app     // Это позволяет эффективно управлять объектами и выполнять выборку на основе меток.

Перед применением нового replica set рекомендуется удалить старые поды.

Применить(создать, запустить) replica set:
kubectl create -f replicaset.yml

сокращение:
replica set - rs

kubectl get rs будет работать так же, как и kubectl get replica set


Масштабирование replica set
Первый способ:
1) Обновить количество реплик в конфигурационном файле replica-set.yaml
2) kubectl apply -f replicaset.yaml
apply != create

Второй способ:
kubectl scale replicaset my-replicaset --replicas 3


Обновление версий приложений в Kubernetes
Первый способ:
1) внести изменеия в в файл replicaset.yaml
2) kubectl apply -f replicaset.yaml

Второй способ:
kubectl edit replicaset my-replicaset
Таким образом, изменения вносятся непосредственно в кластер в реальном времени, но применены они не будут, для применения нужно что бы под пересоздался.
При использовании команды edit изменения вносятся в удаленный объект в кластере и не сохраняются в локальном файле конфигурации, таком как replicaset.yaml

Третий способ:
kubectl set image replicaset my-replicaset nginx=nginx:1.21

поды не были обновлены автоматически, поскольку replica set предназначен для поддержания заданного количества реплик, но не для обновления уже существующих подов после изменения темплейта

Получение подробной информации о поде:
kubectl describe pod <podname>
Получение подробной информации о replicaset:
kubectl describe replicaset <replicasetName>

Можно сделать describe любого объекта в кластере Kubernetes. Эта команда предоставляет базовое описание объекта, включая информацию о полях и событиях (events), произошедших с объектом в кластере Kubernetes.

kubectl get pod <podname> -o=jsonpath='{.spec.containers[*].image}{"\n"}'
Ключ -o jsonpath позволяет получить не весь объект целиком, а только содержимое конкретных полей.
Он крайне полезен при написании скриптов для автоматизации задач в Kubernetes.


2.2.3 Практика
  860  cd k8s_slurm/slurm_devk8s/practice/2.application-abstractions/2.replicaset
  863  kubectl apply -f replicaset.yaml
  865  kubectl scale rs my-replicaset --replicas 3
  867  kubectl delete pod my-replicaset-rvxpn
  869  kubectl set image rs my-replicaset nginx=nginx:1.21
  870  kubectl describe rs my-replicaset
  872  kubectl describe pod my-replicaset-bj8tl
  873  kubectl delete pod my-replicaset-bj8tl
  874  kubectl get pod
  875  kubectl describe pod my-replicaset-bwsm2
  876  kubectl get pod my-replicaset-bwsm2 -o=jsonpath='{.spec.containers[*].image}{"\n"}'
  877  kubectl delete rs --all
  878  kubectl get rs
  879  kubectl get pods







2.3.1 Deployment 

Deployment представляет собой уровень абстракции над ReplicaSet и используется для управления обновлениями версий приложений. Основная задача Deployment — создание новых ReplicaSet с обновленными версиями приложения, которые, в свою очередь, создают необходимые поды.


Проверим создание Deployment, применив команду:
kubectl get deployment

Чтобы изменить число реплик, можно отредактировать соответствующий файл Deployment или применить команду:
kubectl scale


Теперь попробуем обновить наш Deployment. Для этого существует несколько способов:
1 Изменение версии образа в файле с описанием Deployment и его применение через команду kubectl apply
2 Использование команды kubectl edit deployment my-deployment для редактирования ресурса непосредственно в кластере:
3 Использование команды kubectl set image deployment my-deployment 'nginx=nginx:1.13'

Стратегия Rolling Update
Rolling Update — стратегия обновления, поддерживаемая Kubernetes по умолчанию. Она обеспечивает постепенное обновление подов, гарантируя плавный переход между версиями приложения. Суть Rolling Update заключается в том, что старые реплики заменяются новыми поэтапно: сначала часть старых реплик отключается, затем на их месте запускаются новые. Такой процесс не прерывает работу приложения и остается незаметным для пользователей, поскольку всегда есть активные поды, обрабатывающие запросы.
Процесс управления Rolling Update в Deployment позволяет контролировать, сколько старых реплик удаляется за раз и сколько новых запускается одновременно. Эти параметры могут быть настроены в конфигурации Deployment.
В описании конфигурации Deployment: (spec) можно задать дополнительные параметры, такие как стратегия обновления (strategy) это позволяет управлять процессом обновления приложения, задавая количество одновременно обновляемых и сохраняемых реплик во время update. В поле strategy у Deployment можно задать два типа стратегий обновления: RollingUpdate и Recreate

Тип RollingUpdate, как уже было рассмотрено ранее, используется по умолчанию и подразумевает постепенное обновление подов: сначала создаются новые поды, а затем старые поды удаляются. Этот метод обновления обеспечивает непрерывность работы приложения, хотя требует дополнительных ресурсов, так как некоторое время новые и старые поды будут работать одновременно.

Внутри стратегии RollingUpdate предусмотрены два важных параметра: maxSurge и MaxUnavailable

MaxSurge определяет, на сколько новых реплик можно увеличить текущее количество подов относительно заданного значения replicas во время обновления. Например, если у нас replicas равно 2, то при MaxSurge: 1 Kubernetes может поднять одну дополнительную реплику во время обновления. В итоге, на время обновления у нас будет три работающих пода — это текущие 2 пода плюс одна дополнительная реплика.
MaxUnavailable указывает, сколько подов может быть недоступно во время обновления. В нашем случае, если MaxUnavailable: 1, это означает, что как только процесс обновления запустится, Kubernetes может сразу же удалить одну старую реплику, оставив временно только одну рабочую.

Вместо числовых значений, можно указать проценты. Например, можно настроить MaxSurge и MaxUnavailable как 10%

Стратегия Recreate, в свою очередь, предполагает полное удаление всех старых подов перед созданием новых. Это подход, при котором приложение будет временно недоступно (downtime), что делает его менее подходящим для production-сред. Однако такой способ может оказаться полезным в ситуациях, когда недопустима одновременная работа старой и новой версий приложения, например, из-за различий в обработке данных. Он также может быть полезен в dev-средах, где важна экономия ресурсов, поскольку в процессе обновления не требуется дополнительного места для одновременной работы двух наборов подов.

Если необходимо использовать стратегию обновления, но формат заполнения поля strategy неизвестен, Kubernetes предоставляет удобный инструмент для получения информации — команду kubectl explain. Она позволяет выводить описание полей любых объектов в кластере.
kubectl explain deployment.spec.strategy....

kubectl explain - это man по k8s

INFO
PODs и ReplicaSet не используются напрямую, они являются больше служебными абстракциями
Конекретно используется: Deployment, которы включается в себя ReplicaSet, в свою очередь, который включает в себя PODs

kubectl apply лучше использовать всегда (даже вместо kubectl create)


2.3.3 Практика

#  887  minikube start --driver=vmware
  888  history 1 | tail -n 100
  889  history | tail -n 100
  890  vim deployment.yaml
  891  kubectl apply -f deployment.yaml
  892  kubectl get pods
  894  kubectl set image deployment my-deployment nginx=nginx:1.21
  895  kubectl get pod
  896  kubectl get rs
  897  kubectl get deployments
  898  kubectl describe pod my-deployment-59659b5568-55czr
  899  kubectl get deployment my-deployment -o=jsonpath='{.status.conditions[1].message}{"\n"}'
  902  kubectl delete deployments.apps my-deployment


2.3.4 Практика

cp deployment.yaml deployment-update.yaml
vim deployment-update.yaml

---
# file: practice/2.application-abstractions/3.deployment/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
...

kubectl apply -f deployment-update.yaml
kubectl get pod
vim deployment-update.yaml
kubectl apply -f deployment-update.yaml
kubectl get pod
kubectl get deployment my-deployment -o custom-columns='NAME:.metadata.name,MAXSURGE:.spec.strategy.rollingUpdate.maxSurge,MAXUNAVAILABLE:.spec.strategy.rollingUpdate.maxUnavailable'
kubectl delete deployments.apps my-deployment

Это еще одна возможность ключа -o. Она позволяет вывести описание объекта с пользовательским набором полей.





2.4.1 Namespaces

Namespace — это базовая абстракция кластера Kubernetes, представляющая собой пространство имен. Основная функция namespace заключается в разделении имен объектов внутри кластера. В пределах одного namespace невозможно создать два объекта одного типа с одинаковым именем. 


По умолчанию работа в кластере ведется в namespace Default. 

для указания другого namespace, можно использовать команду: 
kubectl -n
при выполнении команд get, create, apply, delete и других и работать с объектами в другом namespace. Таким образом, namespace обеспечивает разделение пространства имен.


2.4.3 Практика

kubectl -n kube-system get pod
kubectl create ns student
kubectl -n student apply -f deployment.yaml
kubectl -n student get pod

kubectl delete deployments.apps -n student --all

kubectl delete deployment --all






2.5.1 Resources

В Kubernetes ресурсы делятся на два типа:

    Реквесты (Requests) — это минимальное количество ресурсов, которое будет зарезервировано под приложение в кластере. Эти ресурсы гарантированно будут доступны приложению.
    Лимиты (Limits) — это максимальное количество ресурсов, которое под может использовать. Лимиты устанавливают верхнюю границу, выше которой ресурсы не будут выделяться.

При превышении лимитов, например, при попытке приложения использовать больше оперативной памяти, включается механизм Out of Memory Killer (OOM Killer) для контейнера

В соотвествии с указанными Requests кубер раскидывает pods по workers кластера

Проц: 1 ядро = 1000m (мили цпу)
cpu: 100m это 1\10 одного ядра (процессорного времени)

Память: 1Gi = 1024 мегабайт
        1GB = 1000 мегабайт

По соотношению request и limmit кубер пристваивает QoS Class, который можно увидеть в describe

QoS Class:
Garanteed - request = limmit (самый приоритетный)
Этот класс применяется, когда реквесты и лимиты равны между собой для CPU и памяти. В таком случае поду гарантируется выделение всех запрошенных ресурсов. Поды с классом Guaranteed имеют наивысший приоритет и сохраняются на ноде в первую очередь при возникновении дефицита ресурсов.

Burstable - request < limmit
Этот класс применяется, когда реквесты меньше, чем лимиты. В некоторых случаях под может использовать и больше ресурсов, если они доступны. Например, если приложение зарезервировало себе 100 mCPU, но при наличии свободных ресурсов оно может использовать до 200 mCPU, если в лимите указано это значение. Однако такие поды имеют более низкий приоритет, чем Guaranteed, то есть при нехватке ресурсов на ноде Kubernetes сначала начнет переселять поды класса Burstable на другие ноды, в то время как поды класса Guaranteed будут сохраняться дольше всего.

Bestapport - no limmit
Этот класс назначается, когда лимиты не заданы. Приложение может использовать любые доступные ресурсы, но при этом имеет наименьший приоритет. В случае дефицита ресурсов на ноде, поды класса Best Effort будут переселяться в первую очередь.


info:
kubectl delete all --all-namespaces --all   =   kubectl delete all -A --all   =   rm -rf /



2.5.3 Практика

cd ~/slurm/practice/2.application-abstractions/4.resources/
kubectl apply -f deployment-with-resources.yaml
kubectl get pod
kubectl patch deployment my-deployment --patch '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","resources":{"requests":{"cpu":"10"},"limits":{"cpu":"10"}}}]}}}}'
kubectl get pod
kubectl describe pod my-deployment-5b98d9cdff-jnwqv

kubectl delete deployment --all





3.1.1 Переменные окружения

Это первый способ конфигурирования приложения

В рамках деплоймента можно передавать настройки приложению, а именно конфигурировать переменные окружения. Для этого у деплоймента есть конкретная секция, называемая env. В этой секции можно перечислить любое количество переменных окружения и задать им значения, например, DB_HOST, DB_PORT, DB_USER, DB_PASSWORD и так далее.
Конфигурирование происходит очень просто и понятно. В секции env обязательно должны быть две строки: name и value. В name указывается название переменной окружения, например, DB_HOST. В нашем случае название переменной ‘TEST’.  В value — значение этой переменной, например, IP-адрес или доменное имя. Если имя переменной окружения DB_USER, то в value пишется имя этого пользователя и т.д.

env:
- name: foo
  value: bar


Практика 3.1.3
kubectl apply -f deployment-with-env.yaml
kubectl get pods
kubectl describe pod my-deployment-77f7c7bcf5-vblld
kubectl exec -it my-deployment-77f7c7bcf5-vblld -- env
kubectl delete deployments.apps --all







3.2.1 ConfigMAp

Это второй способ конфигурирования приложения

Один configMap можно подключать к различным deployments (например: распространить настройки на другие приложения)

configmap записываются в отдельный *.yaml файл и подключаются в нужный файл приложения (deployment.yaml). Для подключения ConfigMap к приложению используется поле EnvFrom.

Env и ConfigMap можно использовать совместно


cat configmap.yaml                      ✔ 
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap-env
data:
  dbhost: postgresql
  DEBUG: "false"
...

cat deployment-with-env-cm.yaml         ✔ 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        env:
        - name: TEST
          value: foo
        envFrom:                           #подключаем configMap файл
        - configMapRef:
            name: my-configmap-env
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
...


kubectl create -f configmap.yaml
kubectl get cm                 #cm - configmap - onfigmaps
kubectl describe cm my-configmap-env

kubectl get cm my-configmap-env -o yaml

kubectl get deployments.apps

Если внести изменения в configMap и даже применить их, они в уже работающих deployments не отразятся без рестарта


Практика 3.2.3

kubectl create -f configmap.yaml
kubectl get configmaps
kubectl describe cm my-configmap-env
kubectl get cm my-configmap-env -o yaml
kubectl apply -f deployment-with-env-cm.yaml
kubectl exec -it my-deployment-7b5b4c5c99-tq97j -- env
kubectl delete deployments.apps --all






3.3.1 Secrets


Secret поддерживает несколько типов для разных задач:
Generic Secret — используется для хранения токенов, паролей и другой конфиденциальной информации.
Docker-registry Secret — специальный тип для передачи данных авторизации в Docker Registry
TLS Secret — используется для хранения сертификатов для подключения TLS, например, для установления HTTPS-соединений.

Создание секрета:

1 вариант
kubectl create secret generic mysecrets --from-literal='foo=bar' --from-literal='dbpasswd=1q2w3e'

Смотрим какие есть секреты:
kubectl get secrets
kubectl describe secrets  mySecrets
kubectl get secrets mysecrets -o yaml

Секреты выводятся закодированными в Base64


Для надёжного хранения секретов надо использовать vault


Подключение секретов:
Secret можно подключить двумя способами:

1 Целиком — с использованием секции envFrom, что позволит подключить весь Secret целиком ко всем переменным окружения.
2 Избирательно — с использованием секции valueFrom, что позволяет передать значение из Secret только для конкретной переменной.

В манифесте 2:
env:
- name: myVar
  valueFrom:
    secretKeyRef:
      name: mySecrets
      key: foo


В манифесте 1:
envFrom:
  - secretRef:
      name: intercom-secret
  - secretRef:
      name: paypal-secret
  - secretRef:
      name: postgres-secret
  - secretRef:
      name: redis-secret




2 вариант

secret-base64.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: test-secret
data:
  username: bXktYXBw
  password: Mzk1MjgkdmRnN0pi



secret-plaintext.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: test-secret
stringData:
  username: paul
  password: mYSuPeRPass123





stringData - plain text format
data - base64 format



kubectl apply -f secret.yaml




3.3.1 Практика

kubectl create secret generic mySecrets --from-literal='foo=bar' --from-literal='dbpasswd=1q2w3e'
kubectl get secrets
kubectl get secrets -o yaml
kubectl get secret mysecrets -o yaml
kubectl apply -f deployment-with-secret.yaml
kubectl get pod
kubectl describe pod my-deployment-5bf6449f4-jhws8
kubectl apply -f secret.yaml
kubectl get secrets -o yaml






3.4.1 configMap. Часть 2

Актуально для Legacy приложений, которые не воспринимают переменными, например: конфигурация требуется отдельным форматом, отличном от YAML, JSON или ключ-значение

Пример такого configMap:
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  default.conf: |
    server {
        listen       80 default_server;
        server_name  _;

        default_type text/plain;

        location / {
            return 200 '$hostname\n';
        }
    }
...


Пример подключения такого configMap

cat deployment-with-configmap.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/conf.d/
      volumes:
      - name: config
        configMap:
          name: my-configmap
...


Секция Volumes предназначена для подключения различных томов (volumes) к приложению (deployment)
Секция Volumes находится на уровне spec (спецификации) всего деплоймента, а не внутри контейнеров. Это связано с тем, что один и тот же деплоймент может использовать несколько томов и контейнеров одновременно. В итоге, можно подключать разные типы томов к различным контейнерам, а также использовать один и тот же том в нескольких контейнерах. Поэтому тома конфигурируются отдельно в секции Volumes, где описываются все используемые тома, их типы и названия.

VolumeMounts — это секция, которая непосредственно связывает тома из секции Volumes с конкретными контейнерами. В этой секции указывается, какой именно том используется и куда он будет монтирован внутри контейнера.

Volumes - что монтируем
VolumeMounts - кому и куда монтируем

может быть использован не только ConfigMap, но и другие типы томов, включая Secrets

файл default.conf — это символическая ссылка, которая указывает на другой файл, data-default.conf. При этом ..data — это символическая ссылка, ведущая к директории с таймстемпом, поэтому подключенные данные через Volume, можно изменять на лету (без перезагрузки POD), однако некоторые приложения требую рестарт, что бы применить изменённые напирмер конфиги. Изменения происходят за счет симлинков

Проброс порта
kubectl port-forward my-deployment-543fgfdgdf-fsd43d 8080:80 &
curl localhost:8080
Это нужно если необходимо получить данные из контейнера (curl), который находится не в одной сети с кластером
Отключить: fg bg или kill -9 $(ps -ef | grep 'port-forward' | awk '{print $2}')



3.4.3 Практика

kubectl apply -f configmap.yaml
kubectl apply -f deployment-with-configmap.yaml
kubectl get pods  -o wide
curl 10.224.0.34
kubectl describe cm my-configmap
kubectl describe pod my-deployment-678fc95fc8-hfdd5
kubectl exec -it my-deployment-678fc95fc8-hfdd5 -- bash
kubectl port-forward my-deployment-678fc95fc8-hfdd5 8080:80 &
curl localhost:8080


3.4.6 Практика

vim configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  default.conf: |
    server {
        listen       80 default_server;
        server_name  _;

        default_type text/plain;

        location / {
            return 200 '$hostname\nOK\n';
        }
    }
...

curl localhost:8080
kubectl exec -it my-deployment-678fc95fc8-hfdd5 -- bash
cat /etc/nginx/conf.d/default.conf



3.4.7 Практика (херня полная, вообще непонятно)

kubectl apply -f php-config.yaml
kubectl apply -f deployment-with-configmap-php.yaml
kubectl get pods
cat php-config.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-php-conf
data:
  php.ini: |
    [php]
    register_globals = off
    track_errors = yes
...

cat deployment-with-configmap-php.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: quay.io/testing-farm/nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/conf.d/
        - name: php
          mountPath: /etc/php.ini
          subPath: php.ini
      volumes:
      - name: config
        configMap:
          name: my-configmap
      - name: php
        configMap:
          name: my-php-conf
...
kubectl exec -it my-deployment-798c5bd979-ks68h -- bash
ls -lsa /etc          # тут типо что-то не так
kubectl edit deployments my-deployment   # типо чиним Убираем subPath, меняем точку монтирования.   
  - mountPath: /etc
    name: php

kubectl get pod   #под не поднимается 
kubectl describe pod my-deployment-6f74684dd-wd46w
kubectl delete deployments my-deployment






3.5.1 Downward API

Этот метод не столько предназначен для внесения пользовательских конфигураций, сколько для передачи параметров окружения, в котором работает приложение, включая параметры, касающиеся самого Kubernetes. С помощью Downward API можно передать внутрь контейнера определённые параметры манифеста Kubernetes в виде переменных окружения или файлов.

Для получения какого-либо значения из манифеста, например, имя ноды или другого параметра из Kubernetes, используем следующую конструкцию:

- name: __NODE_NAME

valueFrom: показывает, что значение для переменной мы не задаём вручную, а берём из другого источника.
fieldRef: позволяет сослаться на конкретное поле манифеста.
fieldPath: указывает конкретное место в манифесте в формате YAML, откуда будет взято значение. Это значение берется непосредственно из манифеста.


Если требуется получить данные из более глубоких уровней манифеста, можно использовать точечную нотацию YAML, например, spec.resources.limits, чтобы получить лимиты ресурсов.

Также можно передать в контейнер информацию о сервис-аккаунте, под которым работает под. 
Данный Volume имеет тип downwardAPI
В секции items указываем, какие именно данные будут переданы через этот Volume - 'labels’, содержащий все лейблы манифеста, и 'annotations', в который записываются все аннотации манифеста.
В файле labels все метки, ассоциированные с данным манифестом.
В файле annotations записаны все аннотации, относящиеся к этому поду
Аннотации представляют собой дополнительные метки, используемые для реализации определённого функционала в Kubernetes, используются в Ingress

cat deployment-with-downward-api.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        env:
        - name: TEST
          value: foo
        - name: __NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: __POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: __POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: __POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: __NODE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        - name: __POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/conf.d/
        - name: podinfo
          mountPath: /etc/podinfo
      volumes:
      - name: config
        configMap:
          name: my-configmap
      - name: podinfo
        downwardAPI:
          items:
            - path: "labels"
              fieldRef:
                fieldPath: metadata.labels
            - path: "annotations"
              fieldRef:
                fieldPath: metadata.annotations
...

https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/
apiVersion: v1
kind: Pod
metadata:
  name: kubernetes-downwardapi-volume-example
  labels:
    zone: us-est-coast
    cluster: test-cluster1
    rack: rack-22
  annotations:
    build: two
    builder: john-doe
spec:
  containers:
    - name: client-container
      image: registry.k8s.io/busybox
      command: ["sh", "-c"]
      args:
      - while true; do
          if [[ -e /etc/podinfo/labels ]]; then
            echo -en '\n\n'; cat /etc/podinfo/labels; fi;
          if [[ -e /etc/podinfo/annotations ]]; then
            echo -en '\n\n'; cat /etc/podinfo/annotations; fi;
          sleep 5;
        done;
      volumeMounts:
        - name: podinfo
          mountPath: /etc/podinfo
  volumes:
    - name: podinfo
      downwardAPI:
        items:
          - path: "labels"
            fieldRef:
              fieldPath: metadata.labels
          - path: "annotations"
            fieldRef:
              fieldPath: metadata.annotations


https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/
apiVersion: v1
kind: Pod
metadata:
  name: dapi-envars-fieldref
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "sh", "-c"]
      args:
      - while true; do
          echo -en '\n';
          printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE;
          printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;
          sleep 10;
        done;
      env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: MY_POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
  restartPolicy: Never


3.5.3 Практика

kubectl apply -f deployment-with-downward-api.yaml
kubectl exec -it my-deployment-8f7dd6848-kktb6 -- env

kubectl exec -it my-deployment-8f7dd6848-kktb6 -- cat /etc/podinfo/labels
kubectl exec -it my-deployment-8f7dd6848-kktb6 -- cat /etc/podinfo/annotations

kubectl delete deployment my-deployment
kubectl delete configmap my-configmap-env
kubectl delete configmap my-configmap
kubectl delete secret mysecrets








4.1.1 Хранение данные

k8s не для хранения данных. Данные надо хранить в базе данных (отдельные машины). Файлы надо хранить в S3.

пока мы знаем только два типа volumes: confgMap и secret



4.2.1 HotsPath

Это обычное монтирование директории или диска на сервере в контейнер
НО! это опасно!!! При использовании hotsPath пользователь может замонтировать какую угодно директории, например /etc/kubernetes
в котором находятся сертификаты и админские токены...
ДАнный тип нужен для корректного функционирования самого k8s кластера

На PROD это использовать нельзя!

ограничения возможны через Pod Security Admission Controller или Open Policy Agent Gatekeeper, которые позволяют контролировать и ограничивать применение HostPath, оставляя его только для необходимых системных компонентов.

Пример hotsPath


        volumeMounts:
        - name: data
          mountPath: /files
      volumes:
      - name: data
        hostPath:
          path: /data_pod

Каталог с хостовой машины (node (worker) на котором будет запущен Pod) /data_pod будет примонтирован внутри контейнера.
Точка монтирования указана в разделе VolumeMount. Здесь мы указываем, что Volume с именем data следует монтировать в каталог /files внутри контейнера


4.2.3 Практика

cat deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: data
          mountPath: /files
      volumes:
      - name: data
        hostPath:
          path: /data_pod
...



cd ~/slurm/practice/4.saving-data/1.hostpath/
kubectl apply -f deployment.yaml

kubectl get pod
kubectl get all
kubectl describe rs my-deployment-6468989d7d


kubectl delete deployment my-deployment








4.3.1 EmptyDir

В отличии от HostPath, данный тип Volume является более безопасным и чаще применяется в различных сценариях, не вызывая серьезных вопросов, связанных с безопасностью. Volume EmptyDir со всеми данными существует только в течение жизненного цикла пода. Данный тип volume отлично подходит для тестов

Пример:

        volumeMounts:
        - name: data
          mountPath: /files
      volumes:
      - name: data
        emptyDir: {}



4.3.3 Практика

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: data
          mountPath: /files
      volumes:
      - name: data
        emptyDir: {}
...


kubectl apply -f deployment.yaml
kubectl exec -it my-deployment-7db8d46cf-bd75c -- sh -c 'echo "Some data" > /files/data.txt'
kubectl exec -it my-deployment-7db8d46cf-bd75c -- cat /files/data.txt
kubectl delete pod my-deployment-7db8d46cf-bd75c
kubectl exec -it my-deployment-7db8d46cf-bd75c -- sh -c 'cat /files/data.txt'
kubectl delete deployments.apps my-deployment










4.4.1 PV\PVC

PersistentVolumeClaim (PVC) - В рамках Persistent Volume Claim мы описываем требования и пожелания к тому, какой диск или хранилище, волюм нужно подключить к приложению. Например, здесь можно указать:
    Размер: Желаемый объем того Volume, который должен быть доступен в контейнере.
    Тип хранилища: Если в кластере имеется несколько хранилищ, можно выбрать конкретное (Ceph или NFS и т.д.).
    Метод доступа: Способ взаимодействия с подключаемым диском или Volume.
Persistent Volume Claim — это объект, в котором определяются параметры и пожелания к подключаемому диску или Volume
создается отдельно или заранее.


Persistent Volume (PV) - это описание параметров и статуса тома, используемого в кластере

Storage Class - хранит настройки подключения к хранилищу
предоставляет информацию о том, где находится NFS-сервер, какие логины и пароли использовать, как подключаться, и так далее
Это уже уровень ПРОДа


Можно вручную создавать PVs - Pool of PVs, или использовать PV Provisioners - это автоматическое получение томов в k8s ровно необходимого места с СХД
Provisioner следит за запросами PVC. Когда возникает новый запрос, Provisioner автоматически обращается к СХД, создает необходимые тома и подключает их к k8s, формируя PV. Кроме того, Provisioner создаёт тома в точном соответствии с размером, указанным в PVC


Пример создания Storage Class и Volume без использования автоматизации через Provisioner
initContainers — контейнер, который запускается до основных контейнеров и выполняет предварительную настройку

PVC info
Существует два основных типа доступа к Volume:
1. ReadWriteMany — позволяет множественное чтение и запись
2. ReadWriteOnce — ограничивает доступ только для одного источника (поды одной ноды)


SС info
в качестве Provisioner указан NoProvisioner - означаент встроенный, ручное создание PV
Storage Class требует ручного вмешательства, так как он действует по принципу volumeBlindingMode: waitForFirstConsumer


PV info
Volume Mode — File System
Access Mode:
 - ReadWriteOnce

Опция Local указавает, что этот Volume будет использоваться как локальная папка
путь к этой папке через localPath, через объект nodeAffinity и nodeSelector задаем конкретную ноду, на которой будет создан данный Volume

***
Спецификация Storage Class - тут остановился
***

В разделе Data Source укажем StorageClassName (для точной привязки PVC к конкретному Storage Class и, соответственно, к определенному SHD в PVC рекомендуется явно указывать спецификацию Storage Class)

В отличие от hostPath каталоги автоматически не создаются на хостовой ноде, поэтому их необходимо создавать вручную


PV статус 'Bound' означает успешное монтирование



Автоматизация создания PV через NFS
Помимо ручной конфигурации в StorageClass с параметром NoProvisioner, как мы уже рассмотрели, можно настроить автоматическое создание PV через подключение к сетевому хранилищу, например, NFS. В этом случае, когда создается PVC, Provisioner автоматически запрашивает необходимое пространство в хранилище, создаёт PV и монтирует его в кластер. Таким образом, ручные действия не требуются





Пример ручной нарезки дисков

cat pvc.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: fileshare
spec:
  storageClassName: local-storage
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi


cat deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fileshare
spec:
  replicas: 2
  selector:
    matchLabels:
      app: fileshare
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: fileshare
    spec:
      initContainers:
      - image: busybox
        name: mount-permissions-fix
        command: ["sh", "-c", "chmod 777 /data"]
        volumeMounts:
        - name: data
          mountPath: /data
      containers:
      - image: centosadmin/reloadable-nginx:1.12
        name: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/conf.d
        - name: data
          mountPath: /data
      volumes:
      - name: config
        configMap:
          name: fileshare
      - name: data
        persistentVolumeClaim:
          claimName: fileshare


cat configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fileshare
data:
  default.conf: |
    server {
      listen       80 default_server;
      server_name  _;

      default_type text/plain;

      location / {
        return 200 '$hostname\n';
      }

      location /files {
        alias /data;
        autoindex on;
        client_body_temp_path /tmp;
        dav_methods PUT DELETE MKCOL COPY MOVE;
        create_full_put_path on;
        dav_access user:rw group:rw all:r;
      }
    }

cat sc.yaml
---
kind: StorageClass  #ручное создание
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

cat pv.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: node1-pv1
spec:
  capacity:
    storeage: 5Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /local/pv1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1.s0001.slurm.io



kubectl get pvc
kubectl get pv
kubectl get sc

kubectl create -f sc.yaml
kubectl create -f pv.yaml

ssh root@node1.s0001.slurm.io
mkdir -p /local/pv1




4.4.3 Практика

kubectl apply -f pvc.yaml
kubectl get pvc
kubectl get pv
kubectl apply -f .
kubectl describe pod fileshare-9989bfb55-f6q6v
kubectl exec -it fileshare-9989bfb55-f6q6v -- df -h
kubectl delete -f .






4.5.1 Init Container
В секции initContainers можно задать контейнеры, которые будут выполняться перед основными контейнерами, описанными в секции containers. Например, в containers может запускаться наш Nginx, а перед этим, в init-контейнерах, могут выполняться другие задачи. Например, они могут установить необходимые права на каталог, если приложение должно работать от непривилегированного пользователя, или как пример копирование конфигурационных файлов, установка прав или создание каталогов.
Init-контейнеры выполняются последовательно в том порядке, в котором они описаны в манифесте.
init-контейнеры завершают свою работу перед тем, как будут запущены основные контейнеры.






5.1.1 Health Check


Probes
• Liveness Probe
Контроль за состоянием приложения во время его жизни
Исполняется постоянно
• Readiness Probe
Проверяет, готово ли приложение принимать трафик
В случае неудачного выполнения, приложение убирается из балансировки
Исполняется постоянно
• Startup Probe
Проверяет, запустилось ли приложение
Исполняется при старте


Liveness Probe
Liveness проба предназначена для контроля жизнеспособности приложения. Она периодически проверяет, действительно ли приложение работает корректно. Например, каждые 10 секунд эта проба выполняет проверку состояния приложения. Если оно перестало отвечать или функционировать, liveness проба инициирует перезапуск контейнера, что позволяет избежать длительных простоев и зависаний.
Если liveness проба проваливается, это свидетельствует о том, что приложение перестало корректно функционировать. В таком случае Kubernetes перезапускает данный инстанс приложения. Таким образом, liveness проба обеспечивает поддержание работоспособности путем перезапуска зависших или некорректно работающих контейнеров.

Readiness Probe
В отличие от liveness пробы, readiness проба проверяет готовность приложения принимать трафик. Это могут быть две разные проверки, хотя чаще всего это одна проверка, которая прописывается в разных секциях манифеста. Readiness проба также исполняется постоянно с какой-то периодичностью, раз в 10 секунд, например, когда Kubernetes проверяет, готово ли приложение принимать трафик. Если эта проба неудачна, приложение исключается из балансировки, и трафик на эту реплику перестает поступать. Это гарантирует, что пользователи взаимодействуют только с полностью готовыми и работоспособными экземплярами приложения.

Startup Probe
Startup проба проверяет, успешно ли приложение запустилось. Эта проба выполняется единожды, в момент старта контейнера после применения деплоймента и успешного запуска приложения.
Startup проба выполняет определённый холстчек, и если он успешно проходит, можно считать, что приложение готово принимать трафик и другие проверки. Важно отметить, что остальные пробы, такие как liveness проба и readiness проба, начинают выполняться только после успешного завершения Startup пробы.
В Kubernetes startup проба была внедрена для работы с приложениями, которые имеют длительное время запуска, например, устаревшие системы или Java-приложения.


В Kubernetes предусмотрены три вида проверок:

1.     HTTP GET — в этом случае проверка осуществляется с помощью HTTP-запроса. Мы отправляем запрос на заданный путь (path) и порт (port), чтобы проверить, готово ли приложение принимать трафик:
если приложение отвечает кодом 200 на запрос по указанному пути и порту (например, на порт 80), это означает, что оно готово к работе. В случае с Nginx, сервер на порту 80 вернет страницу с приветствием Welcome to Nginx, что подтверждается кодом 200. Стоит отметить, что успешными HTTP-кодами считаются коды в диапазоне от 200 до 399. Например, коды 301 или 302 также считаются успешными, в то время как код 404 указывает на ошибку, и такая проверка будет считаться неудачной.
2.     Exec — этот вид проверки позволяет выполнять команду непосредственно внутри контейнера. Kubernetes заходит внутрь контейнера и исполняет указанную команду. Например, можно выполнить команду SELECT 1 в базе данных, чтобы убедиться, что база данных работает корректно.
3.     TCP-сокет — этот вид проверки просто проверяет доступность указанного сокета. Если сокет доступен, то проверка считается успешной. Это самый простой и базовый тип проверки.

На практике чаще всего используются проверки типа HTTP GET



5.1.3 Практика

cat deployment-with-stuff.yaml
---
# file: practice/5.network-abstractions/1.probes/deployment-with-stuff.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 80
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 80
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
          initialDelaySeconds: 10
        startupProbe:
          httpGet:
            path: /
            port: 80
          failureThreshold: 30
          periodSeconds: 10
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 100m
            memory: 100Mi
...



kubectl apply -f deployment-with-stuff.yaml
kubectl get pod
kubectl describe pod my-deployment-7dcd76b7bb-5qnpt
vim deployment-with-stuff.yaml
kubectl apply -f deployment-with-stuff.yaml
kubectl get pod
kubectl describe pod my-deployment-6d599694c5-4jkjf
kubectl delete deployment --all







5.2.1  Способы публикации приложения в Kubernetes


Основные методы включают использование абстракций Service и Ingress. Эти абстракции предоставляют различные подходы для обеспечения доступа к приложению.
Service представляет собой одну из ключевых абстракций в Kubernetes, обеспечивающую сетевое взаимодействие между подами и внешним миром.

Service в Kubernetes не является прокси-сервером, это просто набор правил IPTables или IPVS, которые обеспечивают маршрутизацию и балансировку трафика между подами в кластере.

Существует несколько типов сервисов:

    ClusterIP
    NodePort
    LoadBalancer
    ExternalName
    ExternalIPs


ClusterIP 
Это тип сервиса, предназначенный для внутреннего сетевого взаимодействия в пределах кластера.

selector указывает, какие поды будут обслуживаться этим сервисом.
type указывает, что сервис является ClusterIP.
ports указывает порты, которые будут использоваться для связи.
ClusterIP сервис предназначен преимущественно для внутреннего использования и обеспечивает связь между различными компонентами приложения внутри кластера.
абстракция Service позволяет обращаться к приложению через DNS-имя, создаваемое на основе имени сервиса.
При создании объекта типа Service автоматически генерируется DNS-запись

Использование Cluster IP для предоставления внешнего доступа к приложению
kubectl port-forward service/myService 10000:80
пробрасывает локальный порт 10000 к порту 80 сервиса myService в кластере.


NodePort
используется для публикации приложений. При применении манифеста для NodePort на каждой ноде кластера (включая рабочие ноды и управляющие компоненты) открывается определённый порт. Этот порт назначается из диапазона 30000–32767. Например, если открыт порт 30000, он будет доступен на всех нодах кластера, и внешние запросы, направленные на этот порт, будут перенаправляться в приложение

Такой подход позволяет открывать порты на уровне нод, что может быть полезно для публикации TCP-сервисов или других нестандартных портов, не связанных с HTTP-трафиком. 

Необходимо указывать порт в URL (например, example.com:30000)



LoadBalancer
Этот тип сервиса предназначен исключительно для использования в облачных средах
Он требует наличия облачного контроллера, который управляет созданием внешнего балансировщика нагрузки в зависимости от настроек сервиса
При использовании сервиса LoadBalancer создаётся внешний балансировщик нагрузки, который распределяет входящий трафик между подами внутри кластера. Этот балансировщик автоматически настраивается в облачной инфраструктуре и обеспечивает доступ к приложению как для HTTP, так и для TCP трафика.



ExternalName
в случае с ExternalName трафик направляется от подов к сервису
При применении такого манифеста создается DNS-запись с именем сервиса. Эта DNS-запись указывает не на поды, а на значение, указанное в поле externalName. В результате при обращении к my-service происходит разрешение DNS на указанный внешний адрес, например, example.com
Применение ExternalName позволяет осуществлять доступ к внешним ресурсам через DNS-записи внутри кластера.



Сервис External IPs
При использовании External IPs создаются правила трансляции, направляющие трафик, поступающий на определенный IP-адрес, в соответствующие поды, то есть в приложение. На каждой ноде кластера устанавливаются сетевые правила, которые перенаправляют трафик, поступающий на указанный IP-адрес, в приложение, например, на 80-й порт.

Сетевые правила создаются на всех нодах кластера, независимо от наличия указанного IP-адреса на конкретной ноде. Обратите внимание на то, что манифест практически точно такой же, как и все предыдущие, за исключением одного нового поля – externalIPs. На этот IP-адрес будет приходить трафик, и с него он будет отправляться в приложение. Администратор должен обеспечить корректное направление трафика на нужную ноду и IP-адрес для доступа к соответствующему приложению.




Headless
Принцип работы данного сервиса заключается в следующем: манифест этого сервиса практически идентичен другим сервисам, за исключением того, что поле ClusterIP установлено в значение none. Это приводит к тому, что при создании сервиса автоматически генерируется DNS-запись с именем.  В этой DNS-записи будут возвращаться все IP-адреса всех подов, относящихся к данному сервису. Таким образом, при запросе типа nsvcap.my-service будут предоставлены IP-адреса всех подов, связанных с этим сервисом.
Если запрос будет направлен к конкретному поду через его имя, например, my-service.имя_пода, будет возвращен конкретный IP-адрес данного пода. Этот тип сервиса находит свое основное применение в базах данных, где необходимо организовать репликацию. В таких сценариях с помощью сервиса можно направлять трафик на запись в один под (например, мастер) и на чтение в другой под (например, реплику), используя возможности DNS-направления.


kubectl get svc



cat clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: my-app
  type: ClusterIP



cat nodeport.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service-np
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: my-app
  type: NodePort





5.2.8 Практика

cat app/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  default.conf: |
    server {
        listen       80 default_server;
        server_name  _;
        default_type text/plain;

        location / {
            return 200 '$hostname\n';
        }
    }


cat app/deployment-with-configmap.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - image: nginx:1.20
        name: nginx
        ports:
        - containerPort: 80
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 80
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 80
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
          initialDelaySeconds: 10
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
          limits:
            cpu: 10m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/conf.d/
      volumes:
      - name: config
        configMap:
          name: my-configmap


kubectl apply -f app/

kubectl get pod
kubectl describe pod my-deployment-


------------------------------------------


cat clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: my-app
  type: ClusterIP


kubectl apply -f clusterip.yaml
kubectl get service
kubectl get pod --show-labels
kubectl describe svc my-service
kubectl run test --image=amouat/network-utils -it bash
# curl my-service
# exit


kubectl get svc  =  kubectl get service


------------------------------------------------------------


cat nodeport.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service-np
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: my-app
  type: NodePort



kubectl apply -f nodeport.yaml
kubectl get nodes
kubectl get svc

curl -i 172.17.81.129:31587
curl -i minikube:31587


---------------------------------------------


cat loadbalancer.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service-lb
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: my-app
  type: LoadBalancer



kubectl create -f loadbalancer.yaml
kubectl get svc

---------------------------------------------

kubectl delete svc my-service-lb my-service-np
kubectl delete deployments.apps --all
kubectl delete pod test







5.3.1 Ingress 


Ingress не является сервисом, это отдельная абстракция. Ingress представляет собой манифест,  Ingress-контроллер – это приложение, работающее в Kubernetes.

Ingress-контроллер представляет собой настроенный по манифесту веб-сервис или балансировщик нагрузки: Nginx, Haproxy или Envoy

Пример ingress манифеста:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
spec:
  rules:
  - host: foo.mydomain.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: my-service
            port:
              number: 80



Добавляем сертификаты:
1.Создаем секрет с сертификатом 
kubectl create secret tls ${CERT_NAME} --key ${KEY_FILE} --cert ${CERT_FILE}
2. kubectl get secrets
apiVersion: v1
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
kind: Secret
metadata:
  name: secret-tls
  namespace: default
type: kubernetes.io/tls

3. Указываем сертификат в Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tls-ingress
spec:
  tls:
  - hosts:
    - sslfoo.com
    secretName: secret-tls



Можно сделать автогенерацию сертификата через letsencrypt
1. Созадём автогенерацию сертификатов
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: hostname-ru
  namespace: default
spec:
  acme:
    config:
    - domains:
      - hostname.ru
      - www.hostname.ru
      http01:
        ingress: ""
        ingressClass: nginx
  secretName: hostname-ru-tls
  commonName: hostname.ru
  dnsNames:
  - hostname.ru
  - www.hostname.ru
  issuerRef:
    name: letsencrypt
    kind: ClusterIssuer

2. Подключаем сертификаты в ingress

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tls-ingress
  annotations:
    kubernetes.io/tls-acme:"true"

ИЛИ

certmanager.k8s.io/cluster-issuer: letsencrypt


Аннотация представляет собой средство передачи дополнительных инструкций контроллеру или приложению, которое обрабатывает данный манифест. В случае с Ingress, взаимодействие происходит с Ingress Controller. Аннотация задает дополнительную логику выполнения, расширяя стандартные возможности манифеста.

Каждый Ingress Controller имеет свой набор поддерживаемых аннотаций, которые можно найти в соответствующей документации

Ingress Controller - это опреденённое количество подов с проксей и ЛБ в своём неймспейсе


5.3.3 Практика (не делал https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/)


cat nginx-ingress.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress-nginx
  annotations:
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
  - host: my.edu.example
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: my-service
            port:
              number: 80


kubectl apply -f nginx-ingress.yaml

kubectl get pod -n ingress-nginx -o wide
kubectl get ing


curl my.edu.example








6 Устройство кластера


6.2.1 ETCD

ETCD поддерживает механизм кворума, что обеспечивает отказоустойчивость кластера
Обычно в кластере Kubernetes развертываются как минимум три реплики ETCD
ETCD — это то, что он является центральным хранилищем данных в Kubernetes
API Server сохраняет и извлекает всю информацию о состоянии объектов кластера, нод и других элементов в\из ETCD


Запускается в контейнере, работает на master-нодах



6.3.1 API Server

API Server представляет собой центральный API Server, с которым взаимодействуют все компоненты кластера.
API Server является единственным компонентом, который напрямую взаимодействует с базой данных ETCD
Он отвечает за чтение и запись данных в ETCD, в то время как все остальные компоненты кластера получают доступ к данным и отправляют их через API Server

API Server отвечает за валидацию данных

Взаимодействовать с API server можно не только классическим способом, но и посредством отправки запросов напрямую, например, с использованием команды curl или через программный код. API Server отвечает за аутентификацию и авторизацию как компонентов самого кластера, так и пользователей, и приложений, работающих в нем.

Запускается в контейнере, работает на master-нодах



6.4.1 Controller-manager

Представляет из себя набор контроллеров за состоянием кластера (ноды, поды, реплика сеты)
В нём есть GarbageCollector

Controller-manager общается с api server (ищет события)

Генерирует описания replicaset'ов и pod'ов из объекта deployment

Запускается в контейнере, работает на master-нодах


6.5.1 Scheduler
Отвечает за назначение подов на ноды

Назначет поды на ноды
Scheduler общается с api server (ищет события)
Делает pod binding

Запускается в контейнере, работает на master-нодах



6.6.1 Kubelet
Kubelet представляет собой агент (сервис), работающий на всех нодах в кластере Kubernetes
Kubelet отвечает за взаимодействие с контейнерным интерфейсом на ноде, таким как Docker или containerd. Это единственный компонент, который не запускается в контейнере Docker в кластере Kubernetes. Вместо этого Kubelet взаимодействует напрямую с контейнерным демоном или любым контейнерным интерфейсом, который запущен на ноде.

Kubelet выполняет создание и управление подами на ноде. Он отвечает за запуск контейнеров, создание необходимых ресурсов и поддержание их состояния в соответствии с указаниями из API Server.

Kubelet также осуществляет регулярную синхронизацию статуса своей ноды с API Server. Он отправляет регулярные сигналы (пинги), подтверждающие, что нода остается доступной и функционирующей.




6.7.1 kube-proxy

kube-proxy работает на всех нодах в кластере Kubernetes

Основная задача kube-proxy заключается в реализации сетевых абстракций кластера, в частности, в управлении сервисами. Он управляет сетевыми правилами на нодах, создавая специальные правила через IPVS или IPTables для обеспечения работы сервисов и маршрутизации трафика.





Заключение:
Все компоненты кластера следят за событиями в kube-API server

































